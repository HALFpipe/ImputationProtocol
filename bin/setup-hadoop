#!/usr/bin/env bash

OPTS=$(getopt -o n:m: --long n-cores:,memory-mb: -n 'setup-hadoop' -- "$@")
eval set -- "$OPTS"

verbose=1

n_cores=$(nproc)
memory_mb=$(free -m | awk '/^Mem:/{print $2}')

while true; do
  case "$1" in
  -n | --n-cores)
    case "$2" in
    "") shift 2 ;;
    *)
      n_cores=$2
      shift 2
      ;;
    esac
    ;;
  -m | --memory-mb)
    case "$2" in
    "") shift 2 ;;
    *)
      memory_mb=$2
      shift 2
      ;;
    esac
    ;;
  --)
    shift
    break
    ;;
  *)
    echo "Internal error!"
    exit 1
    ;;
  esac
done

run_cmd() {
  command="$*"

  printf '%s\n' --------------------

  if [ "${verbose}" = "1" ]; then
    printf "%s\n" "${command}"
  fi

  eval "${command}"

  exit_code=$?
  if [[ $exit_code -gt 0 ]]; then
    fail "ERROR: command exited with nonzero status $exit_code"
  fi

  printf '%s\n' --------------------

  return $exit_code
}

fail() {
  printf '%s\n' "$1" >&2
  exit "${2-1}"
}

# Code starts here.

export HADOOP_HOME=/opt/hadoop
export HADOOP_LIBEXEC_DIR=/opt/hadoop/libexec

export HADOOP_DIR=/data/hadoop
export HADOOP_LOG_DIR=${HADOOP_DIR}/logs
export HADOOP_PID_DIR=${HADOOP_DIR}/pids
export HADOOP_CONF_DIR=${HADOOP_DIR}/config

export HADOOP_DATA_DIR=${HADOOP_DIR}/hdfs
export HADOOP_CACHE_DIR=${HADOOP_DATA_DIR}/cache

# Stop any running processes.
if [ -d ${HADOOP_DIR} ]; then
  for pid_file in "${HADOOP_PID_DIR}"/*.pid; do
    if [ ! -f "${pid_file}" ]; then
      continue
    fi
    if [ -n "$(ps -p "$(cat "${pid_file}")" -o pid=)" ]; then
      run_cmd "kill $(cat "${pid_file}")"
    fi
  done
  if ! rm -rf ${HADOOP_DIR}; then
    fail "ERROR: Failed to remove \"${HADOOP_DIR}\". Please check that there are no running processes that are using \"${HADOOP_DIR}\""
  fi
fi

run_cmd mkdir -p -v ${HADOOP_LOG_DIR} ${HADOOP_PID_DIR} ${HADOOP_CONF_DIR} ${HADOOP_CACHE_DIR}
run_cmd cp -rnv ${HADOOP_HOME}/etc/hadoop/* ${HADOOP_CONF_DIR}

port_count=14
mapfile -t ports < <(
  python <<EOF
import socket
sockets = list()
for i in range(${port_count}):
    s = socket.socket()
    s.bind(('', 0))
    print(s.getsockname()[1])
    sockets.append(s)
for s in sockets:
    s.close()
EOF
)

run_cmd "cat <<EOF > ${HADOOP_CONF_DIR}/core-site.xml
<?xml version=\"1.0\"?>

<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:${ports[0]}</value>
  </property>
</configuration>
EOF"

run_cmd "cat <<EOF > ${HADOOP_CONF_DIR}/hdfs-site.xml
<?xml version=\"1.0\"?>
<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>

<configuration>
  <property>
    <name>dfs.datanode.http.address</name>
    <value>localhost:${ports[1]}</value>
  </property>
  <property>
    <name>dfs.datanode.ipc.address</name>
    <value>localhost:${ports[2]}</value>
  </property>
  <property>
    <name>dfs.datanode.address</name>
    <value>localhost:${ports[3]}</value>
  </property>
  <property>
    <name>dfs.namenode.http-address</name>
    <value>localhost:${ports[4]}</value>
  </property>
  <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>localhost:${ports[5]}</value>
  </property>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <!-- Immediately exit safemode as soon as one DataNode checks in.
       On a multi-node cluster, these configurations must be removed.  -->
  <property>
    <name>dfs.safemode.extension</name>
    <value>0</value>
  </property>
  <property>
     <name>dfs.safemode.min.datanodes</name>
     <value>1</value>
  </property>
  <property>
     <name>hadoop.tmp.dir</name>
     <value>${HADOOP_CACHE_DIR}</value>
  </property>
  <property>
     <name>dfs.namenode.name.dir</name>
     <value>file:///data/hadoop/dfs/name</value>
  </property>
  <property>
     <name>dfs.namenode.checkpoint.dir</name>
     <value>file:///data/hadoop/dfs/namesecondary</value>
  </property>
  <property>
     <name>dfs.datanode.data.dir</name>
     <value>file:///data/hadoop/dfs/data</value>
  </property>
</configuration>
EOF"

# Configure pseudo-distributed operation as per
# https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html

run_cmd "cat <<EOF > ${HADOOP_CONF_DIR}/mapred-site.xml
<?xml version=\"1.0\"?>
<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>

<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
  <property>
    <name>mapreduce.application.classpath</name>
    <value>${HADOOP_HOME}/share/hadoop/mapreduce/*:${HADOOP_HOME}/share/hadoop/mapreduce/lib/*</value>
  </property>
  <property>
    <name>mapred.child.java.opts</name>
    <value>-Xmx1024m</value>
  </property>
  <property>
    <name>mapreduce.task.timeout</name>
    <value>0</value>
  </property>
  <property>
    <name>mapreduce.task.stuck.timeout-ms</name>
    <value>0</value>
  </property>
  <property>
    <name>mapreduce.shuffle.port</name>
    <value>${ports[6]}</value>
  </property>
</configuration>
EOF"

run_cmd "cat <<EOF > ${HADOOP_CONF_DIR}/yarn-site.xml
<?xml version=\"1.0\"?>

<configuration>
  <property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>${n_cores}</value>
  </property>
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>${memory_mb}</value>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-vcores</name>
    <value>${n_cores}</value>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>${memory_mb}</value>
  </property>
  <property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
  </property>
  <property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
  </property>
  <!-- Disable disk space checks -->
  <property>
    <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
    <value>100</value> 
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.env-whitelist</name>
    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>localhost:${ports[7]}</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>localhost:${ports[8]}</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>localhost:${ports[9]}</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>localhost:${ports[10]}</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>localhost:${ports[11]}</value>
  </property>
  <property>
    <name>yarn.nodemanager.webapp.address</name>
    <value>localhost:${ports[12]}</value>
  </property>
  <property>
    <name>yarn.nodemanager.localizer.address</name>
    <value>localhost:${ports[13]}</value>
  </property>
</configuration>
EOF"

run_cmd "sed \
  -i s/org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator/org.apache.hadoop.yarn.util.resource.DominantResourceCalculator/g \
  ${HADOOP_CONF_DIR}/capacity-scheduler.xml"

run_cmd "echo localhost > ${HADOOP_CONF_DIR}/masters"
run_cmd "cat <<EOF >> ${HADOOP_CONF_DIR}/hadoop-env.sh

# Custom

export HADOOP_LOG_DIR=${HADOOP_LOG_DIR}
export HADOOP_PID_DIR=${HADOOP_PID_DIR}
export HADOOP_SECURE_DN_PID_DIR=${HADOOP_PID_DIR}
export HADOOP_HOME_WARN_SUPPRESS=TRUE

export JAVA_HOME=${JAVA_HOME}
EOF"

if [ -d ${HADOOP_DATA_DIR} ]; then
  run_cmd rm -rf ${HADOOP_DATA_DIR}
  run_cmd mkdir -p -v ${HADOOP_DATA_DIR} ${HADOOP_CACHE_DIR}
fi

run_cmd hdfs namenode -format -nonInteractive -force

run_cmd ${HADOOP_HOME}/bin/hdfs --config "${HADOOP_CONF_DIR}" --daemon start namenode
run_cmd ${HADOOP_HOME}/bin/hdfs --config "${HADOOP_CONF_DIR}" --daemon start secondarynamenode
run_cmd ${HADOOP_HOME}/bin/hdfs --config "${HADOOP_CONF_DIR}" --daemon start datanode

if [ -z "$USER" ]; then
  USER=$(whoami)
fi
if [[ $(id -u) -eq 0 ]]; then # running as root
  export HADOOP_JOBTRACKER_USER="mapred"
  export HADOOP_TASKTRACKER_USER="mapred"

  run_cmd chmod -R ugo+rw ${HADOOP_LOG_DIR} ${HADOOP_PID_DIR} ${HADOOP_CONF_DIR} ${HADOOP_DATA_DIR} # need to allow non-root users to access
else
  export HADOOP_JOBTRACKER_USER=${USER}
  export HADOOP_TASKTRACKER_USER=${USER}
fi

run_cmd hadoop fs -chmod -R 1777 /

run_cmd hadoop fs -mkdir -p /tmp
run_cmd hadoop fs -chmod -R 1777 /tmp
run_cmd hadoop fs -mkdir -p /var
run_cmd hadoop fs -mkdir -p /var/log
run_cmd hadoop fs -chmod -R 1775 /var/log

run_cmd hadoop fs -mkdir -p "${HADOOP_CACHE_DIR}/${HADOOP_JOBTRACKER_USER}/mapred/staging"
run_cmd hadoop fs -chmod 1777 "${HADOOP_CACHE_DIR}/${HADOOP_JOBTRACKER_USER}/mapred/staging"
run_cmd hadoop fs -chown -R "${HADOOP_JOBTRACKER_USER}" "${HADOOP_CACHE_DIR}/${HADOOP_JOBTRACKER_USER}"

run_cmd hadoop fs -mkdir -p "/user/${USER}"
run_cmd hadoop fs -chown "${USER}" "/user/${USER}"

run_cmd ${HADOOP_HOME}/bin/yarn --daemon start resourcemanager
run_cmd ${HADOOP_HOME}/bin/yarn --daemon start nodemanager
