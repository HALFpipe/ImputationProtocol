#!/usr/bin/env bash

OPTS=`getopt -o n: --long n-cores: -n 'setup-hadoop' -- "$@"`
eval set -- "$OPTS"

verbose=1

N_CORES=$(nproc)

while true ; do
    case "$1" in
        -n|--n-cores)
            case "$2" in
                "") shift 2 ;;
                *) N_CORES=$2 ; shift 2 ;;
            esac ;;
        --) shift ; break ;;
        *) echo "Internal error!" ; exit 1 ;;
    esac
done

run_cmd() {
    cmd="$*"

    printf '%s\n' --------------------

    if [ "${verbose}" = "1" ]; then
	printf "$cmd\n"
    fi

    eval "$@"

    EXIT_CODE=$?

    if [[ $EXIT_CODE -gt 0 ]]; then
	echo "ERROR: command exited with nonzero status $EXIT_CODE"
    fi

    printf '%s\n' --------------------

    return $EXIT_CODE
}

# Code starts here.

source /usr/lib/bigtop-utils/bigtop-detect-javahome

export HADOOP_HOME=/usr/lib/hadoop
export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec

export HADOOP_DIR=/data/hadoop
export HADOOP_LOG_DIR=${HADOOP_DIR}/logs
export HADOOP_PID_DIR=${HADOOP_DIR}/pids
export HADOOP_CONF_DIR=${HADOOP_DIR}/config

export HADOOP_DATA_DIR=${HADOOP_DIR}/hdfs
export HADOOP_CACHE_DIR=${HADOOP_DATA_DIR}/cache

# Stop any running processes.
if [ -d ${HADOOP_DIR} ]; then
    for i in ${HADOOP_PID_DIR}/*.pid
    do
        if [ ! -f "${i}" ]
        then
            continue
        fi
        if [ -n "$(ps -p $(cat ${i}) -o pid=)" ]
        then
            run_cmd "kill $(cat ${i})"
        fi
    done
    run_cmd rm -rf ${HADOOP_DIR}
fi

run_cmd mkdir -p -v ${HADOOP_LOG_DIR} ${HADOOP_PID_DIR} ${HADOOP_CONF_DIR} ${HADOOP_CACHE_DIR}
run_cmd cp -rnv ${HADOOP_HOME}/etc/hadoop/* ${HADOOP_CONF_DIR}

run_cmd "sed -e s/AMOUNT/${N_CORES}/ -e s/HOSTNAME/localhost/ /usr/bin/mapred-site-template.xml > ${HADOOP_CONF_DIR}/mapred-site.xml"

port_count=9
ports=($(python <<EOF
import socket
sockets = list()
for i in range(${port_count}):
    s = socket.socket()
    s.bind(('', 0))
    print(s.getsockname()[1])
    sockets.append(s)
for s in sockets:
    s.close()
EOF
))

run_cmd "sed -e s/AMOUNT/${N_CORES}/ \
    -e s/HOSTNAME/localhost/ \
    -e s/8021/${ports[0]}/ \
    -e \"s|<configuration>|<configuration>\n  <property>\n    <name>mapred.job.tracker.http.address</name>\n    <value>localhost:${ports[1]}</value>\n  </property>\n  <property>\n    <name>mapred.task.tracker.http.address</name>\n    <value>localhost:${ports[2]}</value>\n  </property>|\" \
/usr/bin/mapred-site-template.xml > ${HADOOP_CONF_DIR}/mapred-site.xml"

run_cmd "sed -e s/HOSTNAME/localhost/ \
    -e s/8020/${ports[3]}/ \
/usr/bin/core-site-template.xml > ${HADOOP_CONF_DIR}/core-site.xml"
   
run_cmd "sed -e \"s|<configuration>|<configuration>\n  <property>\n    <name>dfs.datanode.http.address</name>\n    <value>localhost:${ports[4]}</value>\n  </property>\n  <property>\n    <name>dfs.datanode.ipc.address</name>\n    <value>localhost:${ports[5]}</value>\n  </property>\n  <property>\n    <name>dfs.datanode.address</name>\n    <value>localhost:${ports[6]}</value>\n  </property>\n  <property>\n    <name>dfs.namenode.http-address</name>\n    <value>localhost:${ports[7]}</value>\n  </property>\n  <property>\n    <name>dfs.namenode.secondary.http-address</name>\n    <value>localhost:${ports[8]}</value>\n  </property>|\" \
/usr/lib/hadoop/etc/hadoop/hdfs-site.xml > ${HADOOP_CONF_DIR}/hdfs-site.xml"

run_cmd "sed -i s+/var/lib/hadoop-hdfs/cache+${HADOOP_CACHE_DIR}+g ${HADOOP_CONF_DIR}/*"

run_cmd "echo localhost > ${HADOOP_CONF_DIR}/masters"

run_cmd "cat <<EOF >> ${HADOOP_CONF_DIR}/hadoop-env.sh

# custom

export HADOOP_LOG_DIR=${HADOOP_LOG_DIR}
export HADOOP_PID_DIR=${HADOOP_PID_DIR}
export YARN_LOG_DIR=${HADOOP_LOG_DIR} # no effect if using Hadoop 1
export YARN_PID_DIR=${HADOOP_PID_DIR} # no effect if using Hadoop 1
export HADOOP_SECURE_DN_PID_DIR=${HADOOP_PID_DIR}
export HADOOP_HOME_WARN_SUPPRESS=TRUE

export JAVA_HOME=${JAVA_HOME}
EOF"

if [ -d ${HADOOP_DATA_DIR} ]; then
    run_cmd rm -rf ${HADOOP_DATA_DIR}
    run_cmd mkdir -p -v ${HADOOP_DATA_DIR} ${HADOOP_CACHE_DIR}
fi

run_cmd hdfs namenode -format -nonInteractive -force

run_cmd /usr/lib/hadoop/sbin/hadoop-daemon.sh --config "${HADOOP_CONF_DIR}" start namenode
run_cmd /usr/lib/hadoop/sbin/hadoop-daemon.sh --config "${HADOOP_CONF_DIR}" start secondarynamenode
run_cmd /usr/lib/hadoop/sbin/hadoop-daemon.sh --config "${HADOOP_CONF_DIR}" start datanode

if [ -z "$USER" ]; then
    USER=$(whoami)
fi

if [[ $(id -u) -eq 0 ]] ; then  # running as root
    export HADOOP_JOBTRACKER_USER="mapred"
    export HADOOP_TASKTRACKER_USER="mapred"

    run_cmd chmod -R ugo+rw ${HADOOP_LOG_DIR} ${HADOOP_PID_DIR} ${HADOOP_CONF_DIR} ${HADOOP_DATA_DIR}  # need to allow non-root users to access
else
    export HADOOP_JOBTRACKER_USER=${USER}
    export HADOOP_TASKTRACKER_USER=${USER}
fi

run_cmd hadoop fs -chmod -R 1777 /

run_cmd hadoop fs -mkdir -p /tmp
run_cmd hadoop fs -chmod -R 1777 /tmp
run_cmd hadoop fs -mkdir -p /var
run_cmd hadoop fs -mkdir -p /var/log
run_cmd hadoop fs -chmod -R 1775 /var/log

run_cmd hadoop fs -mkdir -p ${HADOOP_CACHE_DIR}/${HADOOP_JOBTRACKER_USER}/mapred/staging
run_cmd hadoop fs -chmod 1777 ${HADOOP_CACHE_DIR}/${HADOOP_JOBTRACKER_USER}/mapred/staging
run_cmd hadoop fs -chown -R ${HADOOP_JOBTRACKER_USER} ${HADOOP_CACHE_DIR}/${HADOOP_JOBTRACKER_USER}

run_cmd hadoop fs -mkdir -p /user/${USER}
run_cmd hadoop fs -chown ${USER} /user/${USER}

run_cmd env HADOOP_HOME=/usr/lib/hadoop-0.20-mapreduce /usr/lib/hadoop/sbin/hadoop-daemon.sh --config "${HADOOP_CONF_DIR}" start jobtracker
run_cmd env HADOOP_HOME=/usr/lib/hadoop-0.20-mapreduce /usr/lib/hadoop/sbin/hadoop-daemon.sh --config "${HADOOP_CONF_DIR}" start tasktracker
